{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "\n",
    "Id = test[\"Id\"]\n",
    "\n",
    "train = train.drop([\"Id\"], axis = 1)\n",
    "test = test.drop([\"Id\"], axis = 1)\n",
    "\n",
    "data = train.append(test, ignore_index = True, sort = False)\n",
    "data = pd.get_dummies(data, dummy_na = True, drop_first = True)\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 289)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace = True)\n",
    "columns = data.columns\n",
    "sale_price = data['SalePrice']\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.235294     0.150685  0.033420     0.666667        0.500   0.949275   \n",
       "1    0.000000     0.202055  0.038795     0.555556        0.875   0.753623   \n",
       "2    0.235294     0.160959  0.046507     0.666667        0.500   0.934783   \n",
       "3    0.294118     0.133562  0.038561     0.666667        0.500   0.311594   \n",
       "4    0.235294     0.215753  0.060576     0.777778        0.500   0.927536   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0      0.883333     0.12250    0.125089         0.0  ...           0.0   \n",
       "1      0.433333     0.00000    0.173281         0.0  ...           0.0   \n",
       "2      0.866667     0.10125    0.086109         0.0  ...           0.0   \n",
       "3      0.333333     0.00000    0.038271         0.0  ...           0.0   \n",
       "4      0.833333     0.21875    0.116052         0.0  ...           0.0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0           0.0          1.0           0.0                    0.0   \n",
       "1           0.0          1.0           0.0                    0.0   \n",
       "2           0.0          1.0           0.0                    0.0   \n",
       "3           0.0          1.0           0.0                    0.0   \n",
       "4           0.0          1.0           0.0                    0.0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                   0.0                   0.0                   1.0   \n",
       "1                   0.0                   0.0                   1.0   \n",
       "2                   0.0                   0.0                   1.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   1.0   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                    0.0                0.0  \n",
       "1                    0.0                0.0  \n",
       "2                    0.0                0.0  \n",
       "3                    0.0                0.0  \n",
       "4                    0.0                0.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['SalePrice'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "train = data.iloc[:1460]\n",
    "test = data.iloc[1460:]\n",
    "test.drop('SalePrice', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['SalePrice']\n",
    "train = train.drop('SalePrice', axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(978, 288)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(288, 144)\n",
    "        self.fc2 = nn.Linear(144, 72)\n",
    "        self.fc3 = nn.Linear(72, 18)\n",
    "        self.fc4 = nn.Linear(18, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.relu(self.fc1(x))\n",
    "        x = nn.relu(self.fc2(x))\n",
    "        x = nn.relu(self.fc3(x))\n",
    "        x = nn.relu(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 50)\n",
    "label_batch = np.array_split(y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to Tensors from numpy arrays \n",
    "\n",
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "    \n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_test = torch.from_numpy(y_test.values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Regressor()\n",
    "ps = model(train_batch[0])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/400..  Training Loss: 11.766..  Test Loss: 9.601.. \n",
      "Epoch: 2/400..  Training Loss: 8.495..  Test Loss: 7.595.. \n",
      "Epoch: 3/400..  Training Loss: 7.001..  Test Loss: 6.484.. \n",
      "Epoch: 4/400..  Training Loss: 6.065..  Test Loss: 5.683.. \n",
      "Epoch: 5/400..  Training Loss: 5.346..  Test Loss: 5.025.. \n",
      "Epoch: 6/400..  Training Loss: 4.724..  Test Loss: 4.450.. \n",
      "Epoch: 7/400..  Training Loss: 4.203..  Test Loss: 3.981.. \n",
      "Epoch: 8/400..  Training Loss: 3.772..  Test Loss: 3.586.. \n",
      "Epoch: 9/400..  Training Loss: 3.404..  Test Loss: 3.241.. \n",
      "Epoch: 10/400..  Training Loss: 3.077..  Test Loss: 2.932.. \n",
      "Epoch: 11/400..  Training Loss: 2.782..  Test Loss: 2.652.. \n",
      "Epoch: 12/400..  Training Loss: 2.515..  Test Loss: 2.397.. \n",
      "Epoch: 13/400..  Training Loss: 2.269..  Test Loss: 2.159.. \n",
      "Epoch: 14/400..  Training Loss: 2.038..  Test Loss: 1.932.. \n",
      "Epoch: 15/400..  Training Loss: 1.798..  Test Loss: 1.679.. \n",
      "Epoch: 16/400..  Training Loss: 1.548..  Test Loss: 1.438.. \n",
      "Epoch: 17/400..  Training Loss: 1.318..  Test Loss: 1.221.. \n",
      "Epoch: 18/400..  Training Loss: 1.113..  Test Loss: 1.029.. \n",
      "Epoch: 19/400..  Training Loss: 0.932..  Test Loss: 0.860.. \n",
      "Epoch: 20/400..  Training Loss: 0.774..  Test Loss: 0.713.. \n",
      "Epoch: 21/400..  Training Loss: 0.639..  Test Loss: 0.592.. \n",
      "Epoch: 22/400..  Training Loss: 0.530..  Test Loss: 0.497.. \n",
      "Epoch: 23/400..  Training Loss: 0.449..  Test Loss: 0.430.. \n",
      "Epoch: 24/400..  Training Loss: 0.397..  Test Loss: 0.390.. \n",
      "Epoch: 25/400..  Training Loss: 0.367..  Test Loss: 0.368.. \n",
      "Epoch: 26/400..  Training Loss: 0.352..  Test Loss: 0.356.. \n",
      "Epoch: 27/400..  Training Loss: 0.344..  Test Loss: 0.349.. \n",
      "Epoch: 28/400..  Training Loss: 0.338..  Test Loss: 0.345.. \n",
      "Epoch: 29/400..  Training Loss: 0.334..  Test Loss: 0.341.. \n",
      "Epoch: 30/400..  Training Loss: 0.330..  Test Loss: 0.337.. \n",
      "Epoch: 31/400..  Training Loss: 0.326..  Test Loss: 0.333.. \n",
      "Epoch: 32/400..  Training Loss: 0.322..  Test Loss: 0.330.. \n",
      "Epoch: 33/400..  Training Loss: 0.319..  Test Loss: 0.326.. \n",
      "Epoch: 34/400..  Training Loss: 0.315..  Test Loss: 0.323.. \n",
      "Epoch: 35/400..  Training Loss: 0.311..  Test Loss: 0.319.. \n",
      "Epoch: 36/400..  Training Loss: 0.307..  Test Loss: 0.316.. \n",
      "Epoch: 37/400..  Training Loss: 0.303..  Test Loss: 0.312.. \n",
      "Epoch: 38/400..  Training Loss: 0.299..  Test Loss: 0.308.. \n",
      "Epoch: 39/400..  Training Loss: 0.295..  Test Loss: 0.305.. \n",
      "Epoch: 40/400..  Training Loss: 0.291..  Test Loss: 0.301.. \n",
      "Epoch: 41/400..  Training Loss: 0.287..  Test Loss: 0.297.. \n",
      "Epoch: 42/400..  Training Loss: 0.283..  Test Loss: 0.294.. \n",
      "Epoch: 43/400..  Training Loss: 0.279..  Test Loss: 0.290.. \n",
      "Epoch: 44/400..  Training Loss: 0.275..  Test Loss: 0.286.. \n",
      "Epoch: 45/400..  Training Loss: 0.271..  Test Loss: 0.282.. \n",
      "Epoch: 46/400..  Training Loss: 0.267..  Test Loss: 0.279.. \n",
      "Epoch: 47/400..  Training Loss: 0.263..  Test Loss: 0.275.. \n",
      "Epoch: 48/400..  Training Loss: 0.259..  Test Loss: 0.271.. \n",
      "Epoch: 49/400..  Training Loss: 0.255..  Test Loss: 0.268.. \n",
      "Epoch: 50/400..  Training Loss: 0.251..  Test Loss: 0.264.. \n",
      "Epoch: 51/400..  Training Loss: 0.247..  Test Loss: 0.261.. \n",
      "Epoch: 52/400..  Training Loss: 0.243..  Test Loss: 0.257.. \n",
      "Epoch: 53/400..  Training Loss: 0.239..  Test Loss: 0.254.. \n",
      "Epoch: 54/400..  Training Loss: 0.236..  Test Loss: 0.251.. \n",
      "Epoch: 55/400..  Training Loss: 0.232..  Test Loss: 0.247.. \n",
      "Epoch: 56/400..  Training Loss: 0.229..  Test Loss: 0.244.. \n",
      "Epoch: 57/400..  Training Loss: 0.226..  Test Loss: 0.242.. \n",
      "Epoch: 58/400..  Training Loss: 0.223..  Test Loss: 0.239.. \n",
      "Epoch: 59/400..  Training Loss: 0.220..  Test Loss: 0.236.. \n",
      "Epoch: 60/400..  Training Loss: 0.217..  Test Loss: 0.234.. \n",
      "Epoch: 61/400..  Training Loss: 0.215..  Test Loss: 0.231.. \n",
      "Epoch: 62/400..  Training Loss: 0.212..  Test Loss: 0.229.. \n",
      "Epoch: 63/400..  Training Loss: 0.210..  Test Loss: 0.227.. \n",
      "Epoch: 64/400..  Training Loss: 0.208..  Test Loss: 0.225.. \n",
      "Epoch: 65/400..  Training Loss: 0.206..  Test Loss: 0.223.. \n",
      "Epoch: 66/400..  Training Loss: 0.204..  Test Loss: 0.221.. \n",
      "Epoch: 67/400..  Training Loss: 0.202..  Test Loss: 0.220.. \n",
      "Epoch: 68/400..  Training Loss: 0.200..  Test Loss: 0.218.. \n",
      "Epoch: 69/400..  Training Loss: 0.199..  Test Loss: 0.216.. \n",
      "Epoch: 70/400..  Training Loss: 0.197..  Test Loss: 0.215.. \n",
      "Epoch: 71/400..  Training Loss: 0.195..  Test Loss: 0.213.. \n",
      "Epoch: 72/400..  Training Loss: 0.194..  Test Loss: 0.212.. \n",
      "Epoch: 73/400..  Training Loss: 0.192..  Test Loss: 0.210.. \n",
      "Epoch: 74/400..  Training Loss: 0.191..  Test Loss: 0.209.. \n",
      "Epoch: 75/400..  Training Loss: 0.189..  Test Loss: 0.208.. \n",
      "Epoch: 76/400..  Training Loss: 0.188..  Test Loss: 0.206.. \n",
      "Epoch: 77/400..  Training Loss: 0.187..  Test Loss: 0.205.. \n",
      "Epoch: 78/400..  Training Loss: 0.185..  Test Loss: 0.204.. \n",
      "Epoch: 79/400..  Training Loss: 0.184..  Test Loss: 0.202.. \n",
      "Epoch: 80/400..  Training Loss: 0.182..  Test Loss: 0.201.. \n",
      "Epoch: 81/400..  Training Loss: 0.181..  Test Loss: 0.200.. \n",
      "Epoch: 82/400..  Training Loss: 0.180..  Test Loss: 0.198.. \n",
      "Epoch: 83/400..  Training Loss: 0.178..  Test Loss: 0.197.. \n",
      "Epoch: 84/400..  Training Loss: 0.177..  Test Loss: 0.196.. \n",
      "Epoch: 85/400..  Training Loss: 0.176..  Test Loss: 0.195.. \n",
      "Epoch: 86/400..  Training Loss: 0.175..  Test Loss: 0.194.. \n",
      "Epoch: 87/400..  Training Loss: 0.173..  Test Loss: 0.192.. \n",
      "Epoch: 88/400..  Training Loss: 0.172..  Test Loss: 0.191.. \n",
      "Epoch: 89/400..  Training Loss: 0.171..  Test Loss: 0.190.. \n",
      "Epoch: 90/400..  Training Loss: 0.170..  Test Loss: 0.189.. \n",
      "Epoch: 91/400..  Training Loss: 0.168..  Test Loss: 0.188.. \n",
      "Epoch: 92/400..  Training Loss: 0.167..  Test Loss: 0.187.. \n",
      "Epoch: 93/400..  Training Loss: 0.166..  Test Loss: 0.186.. \n",
      "Epoch: 94/400..  Training Loss: 0.165..  Test Loss: 0.185.. \n",
      "Epoch: 95/400..  Training Loss: 0.164..  Test Loss: 0.184.. \n",
      "Epoch: 96/400..  Training Loss: 0.162..  Test Loss: 0.183.. \n",
      "Epoch: 97/400..  Training Loss: 0.161..  Test Loss: 0.182.. \n",
      "Epoch: 98/400..  Training Loss: 0.160..  Test Loss: 0.181.. \n",
      "Epoch: 99/400..  Training Loss: 0.159..  Test Loss: 0.180.. \n",
      "Epoch: 100/400..  Training Loss: 0.158..  Test Loss: 0.179.. \n",
      "Epoch: 101/400..  Training Loss: 0.157..  Test Loss: 0.178.. \n",
      "Epoch: 102/400..  Training Loss: 0.156..  Test Loss: 0.177.. \n",
      "Epoch: 103/400..  Training Loss: 0.155..  Test Loss: 0.177.. \n",
      "Epoch: 104/400..  Training Loss: 0.154..  Test Loss: 0.176.. \n",
      "Epoch: 105/400..  Training Loss: 0.153..  Test Loss: 0.175.. \n",
      "Epoch: 106/400..  Training Loss: 0.152..  Test Loss: 0.174.. \n",
      "Epoch: 107/400..  Training Loss: 0.151..  Test Loss: 0.173.. \n",
      "Epoch: 108/400..  Training Loss: 0.150..  Test Loss: 0.173.. \n",
      "Epoch: 109/400..  Training Loss: 0.149..  Test Loss: 0.172.. \n",
      "Epoch: 110/400..  Training Loss: 0.148..  Test Loss: 0.171.. \n",
      "Epoch: 111/400..  Training Loss: 0.147..  Test Loss: 0.170.. \n",
      "Epoch: 112/400..  Training Loss: 0.146..  Test Loss: 0.170.. \n",
      "Epoch: 113/400..  Training Loss: 0.145..  Test Loss: 0.169.. \n",
      "Epoch: 114/400..  Training Loss: 0.144..  Test Loss: 0.168.. \n",
      "Epoch: 115/400..  Training Loss: 0.143..  Test Loss: 0.168.. \n",
      "Epoch: 116/400..  Training Loss: 0.143..  Test Loss: 0.167.. \n",
      "Epoch: 117/400..  Training Loss: 0.142..  Test Loss: 0.166.. \n",
      "Epoch: 118/400..  Training Loss: 0.141..  Test Loss: 0.166.. \n",
      "Epoch: 119/400..  Training Loss: 0.140..  Test Loss: 0.165.. \n",
      "Epoch: 120/400..  Training Loss: 0.139..  Test Loss: 0.164.. \n",
      "Epoch: 121/400..  Training Loss: 0.138..  Test Loss: 0.164.. \n",
      "Epoch: 122/400..  Training Loss: 0.137..  Test Loss: 0.163.. \n",
      "Epoch: 123/400..  Training Loss: 0.137..  Test Loss: 0.163.. \n",
      "Epoch: 124/400..  Training Loss: 0.136..  Test Loss: 0.162.. \n",
      "Epoch: 125/400..  Training Loss: 0.135..  Test Loss: 0.161.. \n",
      "Epoch: 126/400..  Training Loss: 0.134..  Test Loss: 0.161.. \n",
      "Epoch: 127/400..  Training Loss: 0.133..  Test Loss: 0.160.. \n",
      "Epoch: 128/400..  Training Loss: 0.133..  Test Loss: 0.160.. \n",
      "Epoch: 129/400..  Training Loss: 0.132..  Test Loss: 0.159.. \n",
      "Epoch: 130/400..  Training Loss: 0.131..  Test Loss: 0.159.. \n",
      "Epoch: 131/400..  Training Loss: 0.130..  Test Loss: 0.158.. \n",
      "Epoch: 132/400..  Training Loss: 0.130..  Test Loss: 0.158.. \n",
      "Epoch: 133/400..  Training Loss: 0.129..  Test Loss: 0.157.. \n",
      "Epoch: 134/400..  Training Loss: 0.128..  Test Loss: 0.157.. \n",
      "Epoch: 135/400..  Training Loss: 0.127..  Test Loss: 0.156.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 136/400..  Training Loss: 0.127..  Test Loss: 0.156.. \n",
      "Epoch: 137/400..  Training Loss: 0.126..  Test Loss: 0.155.. \n",
      "Epoch: 138/400..  Training Loss: 0.125..  Test Loss: 0.155.. \n",
      "Epoch: 139/400..  Training Loss: 0.124..  Test Loss: 0.154.. \n",
      "Epoch: 140/400..  Training Loss: 0.124..  Test Loss: 0.154.. \n",
      "Epoch: 141/400..  Training Loss: 0.123..  Test Loss: 0.153.. \n",
      "Epoch: 142/400..  Training Loss: 0.122..  Test Loss: 0.153.. \n",
      "Epoch: 143/400..  Training Loss: 0.122..  Test Loss: 0.152.. \n",
      "Epoch: 144/400..  Training Loss: 0.121..  Test Loss: 0.152.. \n",
      "Epoch: 145/400..  Training Loss: 0.120..  Test Loss: 0.151.. \n",
      "Epoch: 146/400..  Training Loss: 0.120..  Test Loss: 0.151.. \n",
      "Epoch: 147/400..  Training Loss: 0.119..  Test Loss: 0.150.. \n",
      "Epoch: 148/400..  Training Loss: 0.118..  Test Loss: 0.150.. \n",
      "Epoch: 149/400..  Training Loss: 0.118..  Test Loss: 0.149.. \n",
      "Epoch: 150/400..  Training Loss: 0.117..  Test Loss: 0.149.. \n",
      "Epoch: 151/400..  Training Loss: 0.117..  Test Loss: 0.149.. \n",
      "Epoch: 152/400..  Training Loss: 0.116..  Test Loss: 0.148.. \n",
      "Epoch: 153/400..  Training Loss: 0.115..  Test Loss: 0.148.. \n",
      "Epoch: 154/400..  Training Loss: 0.115..  Test Loss: 0.147.. \n",
      "Epoch: 155/400..  Training Loss: 0.114..  Test Loss: 0.147.. \n",
      "Epoch: 156/400..  Training Loss: 0.114..  Test Loss: 0.146.. \n",
      "Epoch: 157/400..  Training Loss: 0.113..  Test Loss: 0.146.. \n",
      "Epoch: 158/400..  Training Loss: 0.112..  Test Loss: 0.146.. \n",
      "Epoch: 159/400..  Training Loss: 0.112..  Test Loss: 0.145.. \n",
      "Epoch: 160/400..  Training Loss: 0.111..  Test Loss: 0.145.. \n",
      "Epoch: 161/400..  Training Loss: 0.111..  Test Loss: 0.145.. \n",
      "Epoch: 162/400..  Training Loss: 0.110..  Test Loss: 0.144.. \n",
      "Epoch: 163/400..  Training Loss: 0.110..  Test Loss: 0.144.. \n",
      "Epoch: 164/400..  Training Loss: 0.109..  Test Loss: 0.143.. \n",
      "Epoch: 165/400..  Training Loss: 0.109..  Test Loss: 0.143.. \n",
      "Epoch: 166/400..  Training Loss: 0.108..  Test Loss: 0.143.. \n",
      "Epoch: 167/400..  Training Loss: 0.108..  Test Loss: 0.142.. \n",
      "Epoch: 168/400..  Training Loss: 0.107..  Test Loss: 0.142.. \n",
      "Epoch: 169/400..  Training Loss: 0.106..  Test Loss: 0.142.. \n",
      "Epoch: 170/400..  Training Loss: 0.106..  Test Loss: 0.141.. \n",
      "Epoch: 171/400..  Training Loss: 0.105..  Test Loss: 0.141.. \n",
      "Epoch: 172/400..  Training Loss: 0.105..  Test Loss: 0.141.. \n",
      "Epoch: 173/400..  Training Loss: 0.104..  Test Loss: 0.140.. \n",
      "Epoch: 174/400..  Training Loss: 0.104..  Test Loss: 0.140.. \n",
      "Epoch: 175/400..  Training Loss: 0.103..  Test Loss: 0.140.. \n",
      "Epoch: 176/400..  Training Loss: 0.103..  Test Loss: 0.139.. \n",
      "Epoch: 177/400..  Training Loss: 0.102..  Test Loss: 0.139.. \n",
      "Epoch: 178/400..  Training Loss: 0.102..  Test Loss: 0.139.. \n",
      "Epoch: 179/400..  Training Loss: 0.102..  Test Loss: 0.139.. \n",
      "Epoch: 180/400..  Training Loss: 0.101..  Test Loss: 0.138.. \n",
      "Epoch: 181/400..  Training Loss: 0.101..  Test Loss: 0.138.. \n",
      "Epoch: 182/400..  Training Loss: 0.100..  Test Loss: 0.138.. \n",
      "Epoch: 183/400..  Training Loss: 0.100..  Test Loss: 0.138.. \n",
      "Epoch: 184/400..  Training Loss: 0.099..  Test Loss: 0.137.. \n",
      "Epoch: 185/400..  Training Loss: 0.099..  Test Loss: 0.137.. \n",
      "Epoch: 186/400..  Training Loss: 0.098..  Test Loss: 0.137.. \n",
      "Epoch: 187/400..  Training Loss: 0.098..  Test Loss: 0.137.. \n",
      "Epoch: 188/400..  Training Loss: 0.097..  Test Loss: 0.136.. \n",
      "Epoch: 189/400..  Training Loss: 0.097..  Test Loss: 0.136.. \n",
      "Epoch: 190/400..  Training Loss: 0.097..  Test Loss: 0.136.. \n",
      "Epoch: 191/400..  Training Loss: 0.096..  Test Loss: 0.136.. \n",
      "Epoch: 192/400..  Training Loss: 0.096..  Test Loss: 0.136.. \n",
      "Epoch: 193/400..  Training Loss: 0.095..  Test Loss: 0.135.. \n",
      "Epoch: 194/400..  Training Loss: 0.095..  Test Loss: 0.135.. \n",
      "Epoch: 195/400..  Training Loss: 0.094..  Test Loss: 0.135.. \n",
      "Epoch: 196/400..  Training Loss: 0.094..  Test Loss: 0.135.. \n",
      "Epoch: 197/400..  Training Loss: 0.094..  Test Loss: 0.135.. \n",
      "Epoch: 198/400..  Training Loss: 0.093..  Test Loss: 0.134.. \n",
      "Epoch: 199/400..  Training Loss: 0.093..  Test Loss: 0.134.. \n",
      "Epoch: 200/400..  Training Loss: 0.092..  Test Loss: 0.134.. \n",
      "Epoch: 201/400..  Training Loss: 0.092..  Test Loss: 0.134.. \n",
      "Epoch: 202/400..  Training Loss: 0.092..  Test Loss: 0.134.. \n",
      "Epoch: 203/400..  Training Loss: 0.091..  Test Loss: 0.134.. \n",
      "Epoch: 204/400..  Training Loss: 0.091..  Test Loss: 0.133.. \n",
      "Epoch: 205/400..  Training Loss: 0.091..  Test Loss: 0.133.. \n",
      "Epoch: 206/400..  Training Loss: 0.090..  Test Loss: 0.133.. \n",
      "Epoch: 207/400..  Training Loss: 0.090..  Test Loss: 0.133.. \n",
      "Epoch: 208/400..  Training Loss: 0.089..  Test Loss: 0.133.. \n",
      "Epoch: 209/400..  Training Loss: 0.089..  Test Loss: 0.133.. \n",
      "Epoch: 210/400..  Training Loss: 0.089..  Test Loss: 0.133.. \n",
      "Epoch: 211/400..  Training Loss: 0.088..  Test Loss: 0.132.. \n",
      "Epoch: 212/400..  Training Loss: 0.088..  Test Loss: 0.132.. \n",
      "Epoch: 213/400..  Training Loss: 0.088..  Test Loss: 0.132.. \n",
      "Epoch: 214/400..  Training Loss: 0.087..  Test Loss: 0.132.. \n",
      "Epoch: 215/400..  Training Loss: 0.087..  Test Loss: 0.132.. \n",
      "Epoch: 216/400..  Training Loss: 0.087..  Test Loss: 0.132.. \n",
      "Epoch: 217/400..  Training Loss: 0.086..  Test Loss: 0.132.. \n",
      "Epoch: 218/400..  Training Loss: 0.086..  Test Loss: 0.132.. \n",
      "Epoch: 219/400..  Training Loss: 0.086..  Test Loss: 0.132.. \n",
      "Epoch: 220/400..  Training Loss: 0.085..  Test Loss: 0.132.. \n",
      "Epoch: 221/400..  Training Loss: 0.085..  Test Loss: 0.132.. \n",
      "Epoch: 222/400..  Training Loss: 0.085..  Test Loss: 0.131.. \n",
      "Epoch: 223/400..  Training Loss: 0.085..  Test Loss: 0.131.. \n",
      "Epoch: 224/400..  Training Loss: 0.084..  Test Loss: 0.131.. \n",
      "Epoch: 225/400..  Training Loss: 0.084..  Test Loss: 0.131.. \n",
      "Epoch: 226/400..  Training Loss: 0.084..  Test Loss: 0.131.. \n",
      "Epoch: 227/400..  Training Loss: 0.083..  Test Loss: 0.131.. \n",
      "Epoch: 228/400..  Training Loss: 0.083..  Test Loss: 0.131.. \n",
      "Epoch: 229/400..  Training Loss: 0.083..  Test Loss: 0.131.. \n",
      "Epoch: 230/400..  Training Loss: 0.082..  Test Loss: 0.131.. \n",
      "Epoch: 231/400..  Training Loss: 0.082..  Test Loss: 0.131.. \n",
      "Epoch: 232/400..  Training Loss: 0.082..  Test Loss: 0.131.. \n",
      "Epoch: 233/400..  Training Loss: 0.082..  Test Loss: 0.131.. \n",
      "Epoch: 234/400..  Training Loss: 0.081..  Test Loss: 0.131.. \n",
      "Epoch: 235/400..  Training Loss: 0.081..  Test Loss: 0.131.. \n",
      "Epoch: 236/400..  Training Loss: 0.081..  Test Loss: 0.131.. \n",
      "Epoch: 237/400..  Training Loss: 0.081..  Test Loss: 0.131.. \n",
      "Epoch: 238/400..  Training Loss: 0.080..  Test Loss: 0.131.. \n",
      "Epoch: 239/400..  Training Loss: 0.080..  Test Loss: 0.131.. \n",
      "Epoch: 240/400..  Training Loss: 0.080..  Test Loss: 0.131.. \n",
      "Epoch: 241/400..  Training Loss: 0.079..  Test Loss: 0.130.. \n",
      "Epoch: 242/400..  Training Loss: 0.079..  Test Loss: 0.130.. \n",
      "Epoch: 243/400..  Training Loss: 0.079..  Test Loss: 0.130.. \n",
      "Epoch: 244/400..  Training Loss: 0.079..  Test Loss: 0.130.. \n",
      "Epoch: 245/400..  Training Loss: 0.078..  Test Loss: 0.130.. \n",
      "Epoch: 246/400..  Training Loss: 0.078..  Test Loss: 0.130.. \n",
      "Epoch: 247/400..  Training Loss: 0.078..  Test Loss: 0.130.. \n",
      "Epoch: 248/400..  Training Loss: 0.078..  Test Loss: 0.130.. \n",
      "Epoch: 249/400..  Training Loss: 0.077..  Test Loss: 0.130.. \n",
      "Epoch: 250/400..  Training Loss: 0.077..  Test Loss: 0.130.. \n",
      "Epoch: 251/400..  Training Loss: 0.077..  Test Loss: 0.130.. \n",
      "Epoch: 252/400..  Training Loss: 0.077..  Test Loss: 0.130.. \n",
      "Epoch: 253/400..  Training Loss: 0.076..  Test Loss: 0.130.. \n",
      "Epoch: 254/400..  Training Loss: 0.076..  Test Loss: 0.130.. \n",
      "Epoch: 255/400..  Training Loss: 0.076..  Test Loss: 0.130.. \n",
      "Epoch: 256/400..  Training Loss: 0.076..  Test Loss: 0.130.. \n",
      "Epoch: 257/400..  Training Loss: 0.075..  Test Loss: 0.130.. \n",
      "Epoch: 258/400..  Training Loss: 0.075..  Test Loss: 0.130.. \n",
      "Epoch: 259/400..  Training Loss: 0.075..  Test Loss: 0.130.. \n",
      "Epoch: 260/400..  Training Loss: 0.075..  Test Loss: 0.130.. \n",
      "Epoch: 261/400..  Training Loss: 0.075..  Test Loss: 0.130.. \n",
      "Epoch: 262/400..  Training Loss: 0.074..  Test Loss: 0.130.. \n",
      "Epoch: 263/400..  Training Loss: 0.074..  Test Loss: 0.130.. \n",
      "Epoch: 264/400..  Training Loss: 0.074..  Test Loss: 0.130.. \n",
      "Epoch: 265/400..  Training Loss: 0.074..  Test Loss: 0.130.. \n",
      "Epoch: 266/400..  Training Loss: 0.073..  Test Loss: 0.130.. \n",
      "Epoch: 267/400..  Training Loss: 0.073..  Test Loss: 0.130.. \n",
      "Epoch: 268/400..  Training Loss: 0.073..  Test Loss: 0.130.. \n",
      "Epoch: 269/400..  Training Loss: 0.073..  Test Loss: 0.130.. \n",
      "Epoch: 270/400..  Training Loss: 0.073..  Test Loss: 0.130.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271/400..  Training Loss: 0.072..  Test Loss: 0.130.. \n",
      "Epoch: 272/400..  Training Loss: 0.072..  Test Loss: 0.130.. \n",
      "Epoch: 273/400..  Training Loss: 0.072..  Test Loss: 0.130.. \n",
      "Epoch: 274/400..  Training Loss: 0.072..  Test Loss: 0.130.. \n",
      "Epoch: 275/400..  Training Loss: 0.072..  Test Loss: 0.130.. \n",
      "Epoch: 276/400..  Training Loss: 0.071..  Test Loss: 0.130.. \n",
      "Epoch: 277/400..  Training Loss: 0.071..  Test Loss: 0.130.. \n",
      "Epoch: 278/400..  Training Loss: 0.071..  Test Loss: 0.130.. \n",
      "Epoch: 279/400..  Training Loss: 0.071..  Test Loss: 0.130.. \n",
      "Epoch: 280/400..  Training Loss: 0.071..  Test Loss: 0.130.. \n",
      "Epoch: 281/400..  Training Loss: 0.070..  Test Loss: 0.130.. \n",
      "Epoch: 282/400..  Training Loss: 0.070..  Test Loss: 0.130.. \n",
      "Epoch: 283/400..  Training Loss: 0.070..  Test Loss: 0.130.. \n",
      "Epoch: 284/400..  Training Loss: 0.070..  Test Loss: 0.130.. \n",
      "Epoch: 285/400..  Training Loss: 0.070..  Test Loss: 0.130.. \n",
      "Epoch: 286/400..  Training Loss: 0.070..  Test Loss: 0.130.. \n",
      "Epoch: 287/400..  Training Loss: 0.069..  Test Loss: 0.130.. \n",
      "Epoch: 288/400..  Training Loss: 0.069..  Test Loss: 0.130.. \n",
      "Epoch: 289/400..  Training Loss: 0.069..  Test Loss: 0.131.. \n",
      "Epoch: 290/400..  Training Loss: 0.069..  Test Loss: 0.131.. \n",
      "Epoch: 291/400..  Training Loss: 0.069..  Test Loss: 0.131.. \n",
      "Epoch: 292/400..  Training Loss: 0.068..  Test Loss: 0.131.. \n",
      "Epoch: 293/400..  Training Loss: 0.068..  Test Loss: 0.131.. \n",
      "Epoch: 294/400..  Training Loss: 0.068..  Test Loss: 0.131.. \n",
      "Epoch: 295/400..  Training Loss: 0.068..  Test Loss: 0.131.. \n",
      "Epoch: 296/400..  Training Loss: 0.068..  Test Loss: 0.131.. \n",
      "Epoch: 297/400..  Training Loss: 0.068..  Test Loss: 0.131.. \n",
      "Epoch: 298/400..  Training Loss: 0.067..  Test Loss: 0.131.. \n",
      "Epoch: 299/400..  Training Loss: 0.067..  Test Loss: 0.131.. \n",
      "Epoch: 300/400..  Training Loss: 0.067..  Test Loss: 0.131.. \n",
      "Epoch: 301/400..  Training Loss: 0.067..  Test Loss: 0.131.. \n",
      "Epoch: 302/400..  Training Loss: 0.067..  Test Loss: 0.131.. \n",
      "Epoch: 303/400..  Training Loss: 0.067..  Test Loss: 0.131.. \n",
      "Epoch: 304/400..  Training Loss: 0.066..  Test Loss: 0.131.. \n",
      "Epoch: 305/400..  Training Loss: 0.066..  Test Loss: 0.131.. \n",
      "Epoch: 306/400..  Training Loss: 0.066..  Test Loss: 0.131.. \n",
      "Epoch: 307/400..  Training Loss: 0.066..  Test Loss: 0.131.. \n",
      "Epoch: 308/400..  Training Loss: 0.066..  Test Loss: 0.131.. \n",
      "Epoch: 309/400..  Training Loss: 0.066..  Test Loss: 0.131.. \n",
      "Epoch: 310/400..  Training Loss: 0.065..  Test Loss: 0.132.. \n",
      "Epoch: 311/400..  Training Loss: 0.065..  Test Loss: 0.131.. \n",
      "Epoch: 312/400..  Training Loss: 0.065..  Test Loss: 0.132.. \n",
      "Epoch: 313/400..  Training Loss: 0.065..  Test Loss: 0.132.. \n",
      "Epoch: 314/400..  Training Loss: 0.065..  Test Loss: 0.132.. \n",
      "Epoch: 315/400..  Training Loss: 0.065..  Test Loss: 0.132.. \n",
      "Epoch: 316/400..  Training Loss: 0.064..  Test Loss: 0.132.. \n",
      "Epoch: 317/400..  Training Loss: 0.064..  Test Loss: 0.132.. \n",
      "Epoch: 318/400..  Training Loss: 0.064..  Test Loss: 0.132.. \n",
      "Epoch: 319/400..  Training Loss: 0.064..  Test Loss: 0.132.. \n",
      "Epoch: 320/400..  Training Loss: 0.064..  Test Loss: 0.132.. \n",
      "Epoch: 321/400..  Training Loss: 0.064..  Test Loss: 0.132.. \n",
      "Epoch: 322/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 323/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 324/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 325/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 326/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 327/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 328/400..  Training Loss: 0.063..  Test Loss: 0.132.. \n",
      "Epoch: 329/400..  Training Loss: 0.062..  Test Loss: 0.132.. \n",
      "Epoch: 330/400..  Training Loss: 0.062..  Test Loss: 0.133.. \n",
      "Epoch: 331/400..  Training Loss: 0.062..  Test Loss: 0.133.. \n",
      "Epoch: 332/400..  Training Loss: 0.062..  Test Loss: 0.133.. \n",
      "Epoch: 333/400..  Training Loss: 0.062..  Test Loss: 0.133.. \n",
      "Epoch: 334/400..  Training Loss: 0.062..  Test Loss: 0.133.. \n",
      "Epoch: 335/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 336/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 337/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 338/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 339/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 340/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 341/400..  Training Loss: 0.061..  Test Loss: 0.133.. \n",
      "Epoch: 342/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 343/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 344/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 345/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 346/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 347/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 348/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 349/400..  Training Loss: 0.060..  Test Loss: 0.133.. \n",
      "Epoch: 350/400..  Training Loss: 0.059..  Test Loss: 0.133.. \n",
      "Epoch: 351/400..  Training Loss: 0.059..  Test Loss: 0.133.. \n",
      "Epoch: 352/400..  Training Loss: 0.059..  Test Loss: 0.133.. \n",
      "Epoch: 353/400..  Training Loss: 0.059..  Test Loss: 0.133.. \n",
      "Epoch: 354/400..  Training Loss: 0.059..  Test Loss: 0.134.. \n",
      "Epoch: 355/400..  Training Loss: 0.059..  Test Loss: 0.134.. \n",
      "Epoch: 356/400..  Training Loss: 0.059..  Test Loss: 0.134.. \n",
      "Epoch: 357/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 358/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 359/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 360/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 361/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 362/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 363/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 364/400..  Training Loss: 0.058..  Test Loss: 0.134.. \n",
      "Epoch: 365/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 366/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 367/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 368/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 369/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 370/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 371/400..  Training Loss: 0.057..  Test Loss: 0.134.. \n",
      "Epoch: 372/400..  Training Loss: 0.056..  Test Loss: 0.134.. \n",
      "Epoch: 373/400..  Training Loss: 0.056..  Test Loss: 0.134.. \n",
      "Epoch: 374/400..  Training Loss: 0.056..  Test Loss: 0.134.. \n",
      "Epoch: 375/400..  Training Loss: 0.056..  Test Loss: 0.134.. \n",
      "Epoch: 376/400..  Training Loss: 0.056..  Test Loss: 0.135.. \n",
      "Epoch: 377/400..  Training Loss: 0.056..  Test Loss: 0.134.. \n",
      "Epoch: 378/400..  Training Loss: 0.056..  Test Loss: 0.135.. \n",
      "Epoch: 379/400..  Training Loss: 0.056..  Test Loss: 0.135.. \n",
      "Epoch: 380/400..  Training Loss: 0.056..  Test Loss: 0.135.. \n",
      "Epoch: 381/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 382/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 383/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 384/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 385/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 386/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 387/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 388/400..  Training Loss: 0.055..  Test Loss: 0.135.. \n",
      "Epoch: 389/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 390/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 391/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 392/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 393/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 394/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 395/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 396/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 397/400..  Training Loss: 0.054..  Test Loss: 0.135.. \n",
      "Epoch: 398/400..  Training Loss: 0.053..  Test Loss: 0.135.. \n",
      "Epoch: 399/400..  Training Loss: 0.053..  Test Loss: 0.135.. \n",
      "Epoch: 400/400..  Training Loss: 0.053..  Test Loss: 0.136.. \n"
     ]
    }
   ],
   "source": [
    "model = Regressor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 400\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_test)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_test)))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.from_numpy(test.values).float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model.forward(test)\n",
    "\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([116608.484, 153003.27 , 185160.61 , ..., 164172.47 , 107129.53 ,\n",
       "       203325.1  ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ravel(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"Id\": Id, \"SalePrice\": np.exp(np.ravel(predictions))})\n",
    "submission.to_csv(\"nn_prices.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
